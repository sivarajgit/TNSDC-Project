# -*- coding: utf-8 -*-
"""chatbot creation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ThD6YrbPMiVON8Ilvxi8Eb3BXPybZBEv
"""

!pip install pydantic

!pip install langchain
!pip install PyPDF

!pip install transformers

!pip install textract

!pip install langchain[all]

!pip install langchain_openai

!pip install faiss-gpu

!pip install faiss-cpu

from IPython.display import display
import ipywidgets as widgets
import os
import textract
import pandas as pd
import matplotlib.pyplot as plt
from transformers import GPT2TokenizerFast
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.chains import ConversationalRetrievalChain
from langchain.llms import OpenAI

# Define the additional processes
def additional_process_1():
    # Add your code for the first additional process here
    print("Executing additional process 1...")
    # Example: Print a message
    print("This is additional process 1.")

def additional_process_2():
    # Add your code for the second additional process here
    print("Executing additional process 2...")
    # Example: Print a message
    print("This is additional process 2.")

# Set OpenAI API key
os.environ["OPENAI_API_KEY"] = "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"

# Load PDF content
loader = PyPDFLoader("/content/attention-is-all-you-need-Paper.pdf")
pages = loader.load_and_split()

# Step 3: Create function to count tokens
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

# Advanced method - Split by chunk

# Step 1: Convert PDF to text
import textract
doc = textract.process("/content/attention-is-all-you-need-Paper.pdf")

# Step 2: Save to .txt and reopen (helps prevent issues)
with open('attention_is_all_you_need.txt', 'w') as f:
    f.write(doc.decode('utf-8'))

with open('attention_is_all_you_need.txt', 'r') as f:
    text = f.read()

# Step 3: Create function to count tokens
tokenizer = GPT2TokenizerFast.from_pretrained("gpt2")

def count_tokens(text: str) -> int:
    return len(tokenizer.encode(text))

# Step 4: Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(
    # Set a really small chunk size, just to show.
    chunk_size = 512,
    chunk_overlap  = 24,
    length_function = count_tokens,
)

chunks = text_splitter.create_documents([text])

# Result is many LangChain 'Documents' around 500 tokens or less (Recursive splitter sometimes allows more tokens to retain context)
type(chunks[0])

# Quick data visualization to ensure chunking was successful
import pandas as pd
import matplotlib.pyplot as plt

# Create a list of token counts
token_counts = [count_tokens(chunk.page_content) for chunk in chunks]

# Create a DataFrame from the token counts
df = pd.DataFrame({'Token Count': token_counts})

# Create a histogram of the token count distribution
df.hist(bins=40, )

# Show the plot
plt.show()

from langchain_openai import OpenAIEmbeddings
# Replace "<your_api_key>" with your actual OpenAI API key
api_key = "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"

# Create an instance of OpenAIEmbeddings with the API key
embeddings = OpenAIEmbeddings(openai_api_key=api_key)


# Create vector database
db = FAISS.from_documents(chunks, embeddings)

# Create conversation chain and chat history
qa = ConversationalRetrievalChain.from_llm(OpenAI(temperature=0.1), db.as_retriever())
chat_history = []

# Function to handle user input
def on_submit(_):
    query = input_box.value
    input_box.value = ""

    if query.lower() == 'exit':
        print("Thank you for using the Transformers chatbot!")
        return
    elif query.lower() == 'process 1':
        additional_process_1()  # Execute additional process 1
    elif query.lower() == 'process 2':
        additional_process_2()  # Execute additional process 2
    elif query.lower() != 'no':
        # Only proceed with question answering if user input is not 'no' or 'exit'
        result = qa({"question": query, "chat_history": chat_history})
        chat_history.append((query, result['answer']))
        display(widgets.HTML(f'<b>User:</b> {query}'))
        display(widgets.HTML(f'<b><font color="blue">Chatbot:</font></b> {result["answer"]}'))

# Display initial message and input box
print("Welcome to the Transformers chatbot! Type 'exit' to stop.")
print("You have two additional processes to choose from: 'process 1' and 'process 2'.")
input_box = widgets.Text(placeholder='Please enter your question or type "exit" to stop:')
input_box.on_submit(on_submit)
display(input_box)